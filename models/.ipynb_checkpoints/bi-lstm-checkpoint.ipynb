{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benza-ord\\source\\repos\\CS583-Research-Project\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 2 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 4)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>result</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>tweet_cleaned</th>\n",
       "      <th>tweet_clst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:28:53-05:00</td>\n",
       "      <td>Kirkpatrick, who wore a baseball cap embroider...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136</td>\n",
       "      <td>['Kirkpatrick', 'wear', 'baseball', 'cap', 'em...</td>\n",
       "      <td>Kirkpatrick who wore a baseball cap embroidere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:04:30-05:00</td>\n",
       "      <td>#&lt;e&gt;obama&lt;/e&gt; debates that Cracker Ass Cracker...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88</td>\n",
       "      <td>['obama', 'debate', 'Cracker', 'Ass', 'Cracker...</td>\n",
       "      <td>obama debates that Cracker Ass Cracker tonight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>09:50:08-05:00</td>\n",
       "      <td>@Hollivan @hereistheanswer  Youre missing the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140</td>\n",
       "      <td>['Hollivan', 'hereistheanswer', 'Youre', 'miss...</td>\n",
       "      <td>Hollivan hereistheanswer Youre missing the poi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:00:16-05:00</td>\n",
       "      <td>I was raised as a Democrat  left the party yea...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146</td>\n",
       "      <td>['raise', 'Democrat', 'leave', 'party', 'years...</td>\n",
       "      <td>I was raised as a Democrat left the party year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>09:48:07-05:00</td>\n",
       "      <td>The &lt;e&gt;Obama camp&lt;/e&gt; can't afford to lower ex...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124</td>\n",
       "      <td>['Obama', 'camp', 'ca', 'afford', 'lower', 'ex...</td>\n",
       "      <td>The Obama camp ca n't afford to lower expectat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date            time  \\\n",
       "0  2012-10-16 00:00:00  10:28:53-05:00   \n",
       "1  2012-10-16 00:00:00  10:04:30-05:00   \n",
       "2  2012-10-16 00:00:00  09:50:08-05:00   \n",
       "3  2012-10-16 00:00:00  10:00:16-05:00   \n",
       "4  2012-10-16 00:00:00  09:48:07-05:00   \n",
       "\n",
       "                                               tweet  class  result  \\\n",
       "0  Kirkpatrick, who wore a baseball cap embroider...      0     NaN   \n",
       "1  #<e>obama</e> debates that Cracker Ass Cracker...      1     NaN   \n",
       "2  @Hollivan @hereistheanswer  Youre missing the ...      0     NaN   \n",
       "3  I was raised as a Democrat  left the party yea...     -1     NaN   \n",
       "4  The <e>Obama camp</e> can't afford to lower ex...      0     NaN   \n",
       "\n",
       "   tweet_length                                      tweet_cleaned  \\\n",
       "0           136  ['Kirkpatrick', 'wear', 'baseball', 'cap', 'em...   \n",
       "1            88  ['obama', 'debate', 'Cracker', 'Ass', 'Cracker...   \n",
       "2           140  ['Hollivan', 'hereistheanswer', 'Youre', 'miss...   \n",
       "3           146  ['raise', 'Democrat', 'leave', 'party', 'years...   \n",
       "4           124  ['Obama', 'camp', 'ca', 'afford', 'lower', 'ex...   \n",
       "\n",
       "                                          tweet_clst  \n",
       "0  Kirkpatrick who wore a baseball cap embroidere...  \n",
       "1  obama debates that Cracker Ass Cracker tonight...  \n",
       "2  Hollivan hereistheanswer Youre missing the poi...  \n",
       "3  I was raised as a Democrat left the party year...  \n",
       "4  The Obama camp ca n't afford to lower expectat...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cxs583_cleaned_file_v2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert tweet_clst to string\n",
    "df['tweet_clst'] = df['tweet_clst'].astype(str)\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "# Max number of words in each tweet.\n",
    "MAX_SEQUENCE_LENGTH = 75\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "RANDOM_SEED = 42\n",
    "# split 90-10 train-test\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18759 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words= MAX_VOCAB_SIZE, filters='#$%&()*+<=>@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
    "tokenizer.fit_on_texts(df_train['tweet_clst'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (10145, 75)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df_train['tweet_clst'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (10145, 3)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df_train['class']).values\n",
    "print('Shape of label tensor:', Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "318/318 [==============================] - 21s 67ms/step - loss: 0.9827 - accuracy: 0.5171\n",
      "Epoch 2/10\n",
      "318/318 [==============================] - 21s 65ms/step - loss: 0.7021 - accuracy: 0.7018\n",
      "Epoch 3/10\n",
      "318/318 [==============================] - 21s 66ms/step - loss: 0.3932 - accuracy: 0.8462\n",
      "Epoch 4/10\n",
      "318/318 [==============================] - 21s 66ms/step - loss: 0.2158 - accuracy: 0.9215\n",
      "Epoch 5/10\n",
      "318/318 [==============================] - 21s 65ms/step - loss: 0.1332 - accuracy: 0.9527\n",
      "Epoch 6/10\n",
      "318/318 [==============================] - 21s 65ms/step - loss: 0.0871 - accuracy: 0.9690\n",
      "Epoch 7/10\n",
      "318/318 [==============================] - 22s 71ms/step - loss: 0.0831 - accuracy: 0.9689\n",
      "Epoch 8/10\n",
      "318/318 [==============================] - 21s 66ms/step - loss: 0.0597 - accuracy: 0.9764\n",
      "Epoch 9/10\n",
      "318/318 [==============================] - 21s 67ms/step - loss: 0.0487 - accuracy: 0.9827\n",
      "Epoch 10/10\n",
      "318/318 [==============================] - 21s 65ms/step - loss: 0.0434 - accuracy: 0.9834\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Embedding, LSTM, SpatialDropout1D, Conv1D, GlobalMaxPooling1D, Bidirectional,TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(MAX_VOCAB_SIZE, 200, input_length=X.shape[1]))\n",
    "#model.add(Bidirectional(LSTM(200)))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(3, activation='relu'))\n",
    "#model.add(Dense(3, activation='sigmoid'))\n",
    "#model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# main model\n",
    "input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "model = Embedding(MAX_VOCAB_SIZE, 200, input_length=MAX_SEQUENCE_LENGTH)(input)\n",
    "model =  Bidirectional (LSTM (100,return_sequences=True,dropout=0.50),merge_mode='concat')(model)\n",
    "model = TimeDistributed(Dense(100,activation='relu'))(model)\n",
    "model = Flatten()(model)\n",
    "model = Dense(100,activation='relu')(model)\n",
    "output = Dense(3,activation='softmax')(model)\n",
    "\n",
    "model = Model(input,output)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# run for small number of epochs then save \n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X, Y, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1128, 75)\n",
      "Shape of label tensor: (1128, 3)\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 3.1580 - accuracy: 0.5328\n",
      "loss 3.1579997539520264\n",
      "accuracy 0.5328013896942139\n"
     ]
    }
   ],
   "source": [
    "X_test = tokenizer.texts_to_sequences(df_test['tweet_clst'].values)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X_test.shape)\n",
    "\n",
    "Y_Test = pd.get_dummies(df_test['class']).values\n",
    "print('Shape of label tensor:', Y_Test.shape)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, Y_Test)\n",
    "\n",
    "print(\"loss\",loss)\n",
    "print(\"accuracy\",accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.58      0.60      0.59       455\n",
      "           0       0.48      0.54      0.51       380\n",
      "           1       0.54      0.41      0.46       293\n",
      "\n",
      "    accuracy                           0.53      1128\n",
      "   macro avg       0.53      0.52      0.52      1128\n",
      "weighted avg       0.53      0.53      0.53      1128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# get the class with highest probability for each sample\n",
    "y_pred = np.argmax(predictions, axis=-1) - 1\n",
    "\n",
    "print(classification_report(df_test['class'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "outF = open(\"obama.txt\", \"w\")\n",
    "STUDENT_ID= 15\n",
    "# PRINT STUDENT ID \n",
    "outF.write(str(STUDENT_ID))\n",
    "outF.write(\"\\n\")\n",
    "\n",
    "for idx, pred in zip(range(len(y_pred)), y_pred):\n",
    "  # write line to output file\n",
    "    outF.write(str(idx)+\";;\"+str(pred))\n",
    "    outF.write(\"\\n\")\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu-kernel",
   "language": "python",
   "name": "keras-gpu-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
