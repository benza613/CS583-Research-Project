{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataPre_processing.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benza613/CS583-Research-Project/blob/main/DataPre_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3fuZP-c9Oht"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive # import drive from google colab\n",
        "\n",
        "ROOT = \"/content/gdrive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCeQdg3F9Wuc"
      },
      "source": [
        "%cd gdrive/MyDrive/DMTM/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seap1FZk9a_P"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuV_lCf29cHX"
      },
      "source": [
        "!pip install tweet-preprocessor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU50-LGC9hMg"
      },
      "source": [
        "import pandas as pd\n",
        "import preprocessor as p\n",
        "import re\n",
        "import string\n",
        "import seaborn as sns\n",
        "import nltk \n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swZwlOiy9iUN"
      },
      "source": [
        "#data=pd.read_excel('training-Obama-Romney-tweets.xlsx',sheet_name='Obama')  \n",
        "DATAFILE = 'training-Obama-Romney-tweets.xlsx'\n",
        "df_obama = pd.read_excel(DATAFILE,sheet_name='Obama', header=None,index_col=None, usecols='B:F', \n",
        "                   skiprows=[0,1], names=['date','time','tweet','class','result'])\n",
        "print(df_obama)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjJqeHVY-FBH"
      },
      "source": [
        "#explore the data\n",
        "print(f'Number of variables - {df_obama.shape[1]}\\n')\n",
        "print(f'Data Types for each variable - \\n{df_obama.dtypes}\\n')\n",
        "print(f'Number of variables for each data type - \\n{df_obama.dtypes.value_counts()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxO2gWij-I7P"
      },
      "source": [
        "sns.countplot(x = 'class', data = df_obama)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QBij-KR5vPG"
      },
      "source": [
        "# Drop rows not having 0, 1, -1\n",
        "df_obama['class'] = df_obama['class'].astype(str)\n",
        "df_obama = df_obama[df_obama['class'].isin(['0', '1', '-1'])] \n",
        "#df_romney = df_romney[df_romney['class'].isin(['0', '1', '-1'])] \n",
        "\n",
        "# Print the shape of the dataframe \n",
        "print(df_obama.shape) \n",
        "sns.countplot(x = 'class', data = df_obama)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLFDwu4F-Vtb"
      },
      "source": [
        "df_obama['tweet'] = df_obama['tweet'].apply(lambda _: str(_))\n",
        "df_obama.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqqQKY550SA6"
      },
      "source": [
        "df_obama['tweet_length'] = df_obama['tweet'].apply(lambda x: len(str(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpMiQ0LKAVLm"
      },
      "source": [
        "df_obama.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaJNbUowCRil"
      },
      "source": [
        "#Basic cleaning using tweet-preprocessor\n",
        "df_obama['tweet'] = df_obama['tweet'].apply(lambda x: p.clean(x))\n",
        "df_obama.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFRw1oCK-bBt"
      },
      "source": [
        "#removing the html tags\n",
        "df_obama['tweet'] = df_obama['tweet'].apply(lambda x: re.sub(re.compile('<[^>]+>'), '', x))\n",
        "df_obama.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXU7PphV-LaF"
      },
      "source": [
        "#removing the extra puncations\n",
        "df_obama['tweet'] = df_obama['tweet'].apply(lambda x: re.sub('[0-9]+', '', \"\".join([char for char in x if char not in string.punctuation])))\n",
        "df_obama.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joYqs5b-zElX"
      },
      "source": [
        "# #Tokenization using tweet-preprocessor\n",
        "# df_obama['tweet_token'] = df_obama['tweet'].apply(lambda x: re.split('\\W+', x))\n",
        "# df_obama.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9-JxsQ91z06"
      },
      "source": [
        "# #remove stop words\n",
        "#stopword = nltk.corpus.stopwords.words('english')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "# df_obama['tweet_token'] = df_obama['tweet'].apply(lambda x: [word for word in x if word not in stopword])\n",
        "# df_obama.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-JhAMrt6WWq"
      },
      "source": [
        "def clean_stopwords(tweet):\n",
        "    tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
        "    clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
        "    clean_s = ' '.join(clean_tokens)\n",
        "    clean_mess = [word for word in clean_s.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\n",
        "    return clean_mess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqS5nCs87T8Q"
      },
      "source": [
        "def normalization(tweet_list):\n",
        "    lem = WordNetLemmatizer()\n",
        "    normalized_tweet = []\n",
        "    for word in tweet_list:\n",
        "        normalized_text = lem.lemmatize(word,'v')\n",
        "        normalized_tweet.append(normalized_text)\n",
        "    return normalized_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBg8Ln9g6YLv"
      },
      "source": [
        "df_obama['tweet_clean'] = df_obama['tweet'].apply(lambda x: clean_stopwords(x))\n",
        "df_obama.head(10)\n",
        "df_obama['tweet_clean'] = df_obama['tweet_clean'].apply(lambda x: normalization(x))\n",
        "df_obama.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64-gy1bQLSd4"
      },
      "source": [
        "def clean_text(text):\n",
        "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
        "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
        "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
        "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFGCK3ZaLtcd"
      },
      "source": [
        "df_obama['tweet_clean']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0MlmQQ5OS4F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv-pmQ2yOTET"
      },
      "source": [
        "**Training and testing data split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdKG1gCEOvcq"
      },
      "source": [
        "df_obama.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luoA_wPUO64b"
      },
      "source": [
        "df_obama.drop(columns=['date', 'time','result','tweet_length','tweet_clean'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kfumzP2LJ3X"
      },
      "source": [
        "CLX_train,CLX_val, CLY_train, CLY_val = train_test_split(df_obama.iloc[:], df_obama['class'], test_size = 0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3BqvwDCPLw0"
      },
      "source": [
        "CLX_train['tweet_clean']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIyX6OfrLJ6M"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "tfidfconverter = TfidfVectorizer(max_features=20000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
        "X = tfidfconverter.fit_transform(df_obama['tweet']).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdisZI9IQGi5"
      },
      "source": [
        "y=df_obama['class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9i6qbn7LKtI"
      },
      "source": [
        "\n",
        "print(tfidfconverter.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VUgSfmwLKyG"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnvBefLDLJ9f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtdOaQkfHi5O"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)  \n",
        "text_classifier.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsvJviH_Qdda"
      },
      "source": [
        "predictions = text_classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C13V0VgQedb"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        " \n",
        "print(confusion_matrix(y_test,predictions))  \n",
        "print(classification_report(y_test,predictions))  \n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8kgdxbTOJ4b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQJ9U-lkOJ9q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FhXl7irT-Co"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGN8vRdxOMww"
      },
      "source": [
        "tokenizer = Tokenizer(num_words= MAX_VOCAB_SIZE, filters='#$%&()*+<=>@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
        "tokenizer.fit_on_texts(df_obama['class'].values + ' DELIM '+ df_obama['tweet'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-tAySTbOZEl"
      },
      "source": [
        "X = tokenizer.texts_to_sequences(df_obama['class'].values + ' DELIM '+ df_obama['tweet'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JwPj8oiOgCp"
      },
      "source": [
        "Y = pd.get_dummies(df_obama['class']).values\n",
        "print('Shape of label tensor:', Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SEK_sBsT-Lw"
      },
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_VOCAB_SIZE = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcoybUIkT-PA"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_VOCAB_SIZE, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "model.add(LSTM(200, dropout = 0.2))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy',f1_m,precision_m, recall_m])\n",
        "\n",
        "# run for small number of epochs then save \n",
        "epochs = 3\n",
        "\n",
        "history = model.fit(X, Y, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}